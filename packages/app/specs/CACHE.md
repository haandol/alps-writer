## âœ… Claude 3.7 í”„ë¡¬í”„íŠ¸ ìºì‹œë¥¼ ìœ„í•œ ìºì‹œí¬ì¸íŠ¸ ì‹œìŠ¤í…œ

- í”„ë¡¬í”„íŠ¸ ìºì‹œë¥¼ ìœ„í•´ ìºì‹œí¬ì¸íŠ¸ë¥¼ ë©”ì‹œì§€ì— ì¶”ê°€í•´ì•¼ í•œë‹¤.
- LLM ì€ íˆ´ì„ ì´ìš©í•˜ì—¬ ëŒ€í™” ì¤‘ì— ì§€ì •ëœ ì„¹ì…˜ì˜ ì‘ì„±ì´ ì™„ë£Œë˜ë©´ section_completed íˆ´ì„ í˜¸ì¶œí•´ì•¼ í•œë‹¤.
- section_completed íˆ´ì€ í•´ë‹¹ ì„¹ì…˜ì˜ ì‘ì„±ì´ ì™„ë£Œë˜ì—ˆê¸° ë•Œë¬¸ì— ìºì‹œí¬ì¸íŠ¸ê°€ ìƒì„±ë  recent_history ì˜ index ë¥¼ cl ì˜ user_session ì— ì €ì¥í•œë‹¤.
- ë‹¤ìŒ LLM ìš”ì²­ ì‹œ í•´ë‹¹ ì¸ë±ìŠ¤ì˜ ìœ„ì¹˜ì— ìºì‹œí¬ì¸íŠ¸ë¥¼ ì¶”ê°€í•´ì¤€ë‹¤.

### ğŸ”§ ì‹œìŠ¤í…œ êµ¬ì„± ìš”ì†Œ
- **LLM ëª¨ë¸**: Claude 3.7 (ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ì§€ì›)
- **íˆ´ ì œê³µ ë°©ì‹**: `bind_tools` (Langchain)
- **íˆ´ ì—­í• **: boolean ê°’ì„ llm ì—ì„œ ì „ë‹¬ë°›ì•„ true ì´ë©´ ìºì‹œí¬ì¸íŠ¸ ìƒì„±
- **ì‹¤ì œ ìºì‹œ ì €ì¥ ìœ„ì¹˜**: ì‚¬ìš©ì ì„¸ì…˜ ë‚´ ìºì‹œ ê´€ë¦¬

---

### âœ… ìºì‹œí¬ì¸íŠ¸ ë¡œì§

| í•­ëª© | ë‚´ìš© |
|------|------|
| ìºì‹œí¬ì¸íŠ¸ ê°œìˆ˜ | ìµœëŒ€ **4ê°œ** (ì´ˆê³¼ ì‹œ FIFO ë°©ì‹ìœ¼ë¡œ ê°€ì¥ ì˜¤ë˜ëœ ê²ƒ ì œê±°) |
| ìºì‹œí¬ì¸íŠ¸ ìœ„ì¹˜ | ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸, ì±•í„° 5 ì™„ë£Œ, ì±•í„° 6 ì™„ë£Œ, ë§ˆì§€ë§‰ ì±•í„° ì™„ë£Œ |
| ìºì‹œ ìƒì„± ì¡°ê±´ | í•´ë‹¹ ì‹œì ì˜ LLM ì‘ë‹µì´ **ì•½ 1800 í† í° ì´ìƒ**ì¼ ê²½ìš°ì—ë§Œ ìƒì„± |
| ìºì‹œ ìœ ì§€ ë°©ì‹ | ì„¸ì…˜ì— ì €ì¥ëœ ìºì‹œë¥¼ ì´í›„ ë©”ì‹œì§€ ë¹Œë“œ ì‹œ í”„ë¡¬í”„íŠ¸ ì•ë‹¨ì— ì‚½ì…í•˜ì—¬ ì»¨í…ìŠ¤íŠ¸ ìœ ì§€ |
| ìºì‹œí¬ì¸íŠ¸ íŠ¸ë¦¬ê±° | Claudeì˜ ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ë§ˆì§€ë§‰ chunkì˜ metadata ë‚´ íˆ´ í˜¸ì¶œ ì—¬ë¶€ë¡œ íŒë‹¨ |

### âœ… ì„¹ì…˜ ì™„ë£Œ íˆ´ ì½”ë“œ

```python
from langchain_core.tools import tool

@tool
def section_completed(section: int) -> bool:
    """
    ì‘ì„±ì´ ì™„ë£Œëœ ì„¹ì…˜ ë²ˆí˜¸ë¥¼ ë°›ì•„ì„œ í•´ë‹¹ ì„¹ì…˜ì˜ ì‘ì„±ì´ ì™„ë£Œë˜ì—ˆìŒì„ í‘œì‹œ

    Args:
        section (int): ì‘ì„±ì´ ì™„ë£Œëœ ì„¹ì…˜ ë²ˆí˜¸, ì˜ˆì‹œ: 5, 6, 11
    Returns:
        bool: ì™„ë£Œ í‘œì‹œ ì„±ê³µ ì—¬ë¶€
    """
```

### âœ… Chainlit ì—ì„œ ìºì‹œí¬ì¸íŠ¸ ì²˜ë¦¬ ë°©ì‹

- Chainlit ì˜ ê²½ìš° ìºì‹œí¬ì¸íŠ¸ê°€ ì¶”ê°€ë  ë•Œ ë§¤ì¹­ë˜ëŠ” cl.Message ì˜ metadata ì— ìºì‹œí¬ì¸íŠ¸ ì •ë³´ë¥¼ ì¶”ê°€í•´ì¤€ë‹¤.
- on_chat_resume hook ì—ì„œ cl.Message ì˜ metadata ì—ì„œ ìºì‹œí¬ì¸íŠ¸ ì •ë³´ë¥¼ í™•ì¸í•´ì„œ ìºì‹œí¬ì¸íŠ¸ë¥¼ ë³µì›í•œë‹¤.

### âœ… ì²˜ë¦¬ íë¦„ ìš”ì•½

1. LLMì´ ì±•í„° ì¢…ë£Œ ë“± íŠ¹ì • ì§€ì ì— ë„ë‹¬í•¨
2. ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ë§ˆì§€ë§‰ chunkì—ì„œ ë©”íƒ€ë°ì´í„° í™•ì¸ â†’ íˆ´ í˜¸ì¶œ í•„ìš” ì—¬ë¶€ íŒë‹¨
3. íˆ´(`bind_tools`)ì€ boolean ê°’ì„ llm ì—ì„œ ì „ë‹¬ë°›ì•„ true ì´ë©´ ìºì‹œí¬ì¸íŠ¸ ìƒì„±
4. ì‹œìŠ¤í…œì€:
   - í˜„ì¬ ì‘ë‹µ í† í° ìˆ˜ â‰¥ 1800 â†’ ìºì‹œ ìƒì„±
   - ì„¸ì…˜ì— ìºì‹œ ì €ì¥ (ìµœëŒ€ 4ê°œ ìœ ì§€)
   - ì´í›„ ë©”ì‹œì§€ ë¹Œë“œ ì‹œ ìºì‹œë¥¼ ìë™ prepend


<example>
  ```python
  from langchain_core.tools import tool
  from langchain_core.messages import HumanMessage, ToolMessage
  from langchain_aws import ChatBedrockConverse
  import asyncio


  @tool
  def save_game_preference(title: str) -> str:
      """ì‚¬ìš©ìê°€ ê²Œì„ì— ëŒ€í•œ ì´ì•¼ê¸°ë¥¼ í•˜ë©´ ê²Œì„ ì œëª©ì„ ì…ë ¥ë°›ì•„ ì‚¬ìš©ìì˜ ê²Œì„ì— ëŒ€í•œ ì„ í˜¸ë„ë¥¼ ì €ì¥í•©ë‹ˆë‹¤.

      Args:
          title (str): ê²Œì„ ì œëª©
      Returs:
          bool: ì„±ê³µ ì—¬ë¶€
      """
      return True


  async def main():
      llm = ChatBedrockConverse(
          model_id="anthropic.claude-3-sonnet-20240229-v1:0",
          region_name="us-east-1",
      )

      tools = [save_game_preference]

      llm_with_tools = llm.bind_tools(tools)

      query = "ì›Œí¬ë˜í”„íŠ¸3 ì¬ë¯¸ìˆë”ë¼. ë‹¤ë¥¸ì‚¬ëŒë“¤ì€ ì–´ë–¤ì§€ ì•Œì•„?"
      print(f"User Query: {query}")

      print("\n--- Streaming Initial LLM Response ---")
      first = True
      ai_msg = None

      async for chunk in llm_with_tools.astream([HumanMessage(content=query)]):
          if first:
              ai_msg = chunk
              first = False
          else:
              ai_msg = ai_msg + chunk

          # ìŠ¤íŠ¸ë¦¬ë°ë˜ëŠ” ë„êµ¬ í˜¸ì¶œ ì •ë³´ ì¶œë ¥
          if ai_msg.tool_calls:
              print(f"Current tool calls: {ai_msg.tool_calls}")
          if ai_msg.content:
              print(f"Current content: {ai_msg.content}")

          await asyncio.sleep(0)  # ë‹¤ë¥¸ ì´ë²¤íŠ¸ ë£¨í”„ ì‘ì—…ì„ ìœ„í•œ ì–‘ë³´

      print("\n--- Complete Initial LLM Response ---")
      print(ai_msg)

      messages = [HumanMessage(content=query), ai_msg]  # Start conversation history

      if not ai_msg.tool_calls:
          print("\n--- LLM did not request any tool calls. Final Answer: ---")
          print(ai_msg.content)
      else:
          for tool_call in ai_msg.tool_calls:
              selected_tool = None
              for t in tools:
                  if t.name == tool_call["name"]:
                      selected_tool = t
                      break

              if selected_tool:
                  tool_output = selected_tool.invoke(tool_call["args"])

                  messages.append(
                      ToolMessage(content=tool_output, tool_call_id=tool_call["id"])
                  )
              else:
                  print(
                      f"Warning: Tool '{tool_call['name']}' requested by LLM but not found."
                  )
                  messages.append(
                      ToolMessage(
                          content=f"Error: Tool '{tool_call['name']}' not found.",
                          tool_call_id=tool_call["id"],
                      )
                  )

          print("\n--- Streaming Final LLM Response ---")
          final_response_content = ""

          async for chunk in llm_with_tools.astream(messages):
              if chunk.content:
                  # contentê°€ ë¦¬ìŠ¤íŠ¸ì¸ ê²½ìš° í…ìŠ¤íŠ¸ ì¶”ì¶œ
                  if isinstance(chunk.content, list):
                      for content_item in chunk.content:
                          if isinstance(content_item, dict) and "text" in content_item:
                              content_text = content_item.get("text", "")
                              final_response_content += content_text
                              print(content_text, end="", flush=True)
                  # contentê°€ ë¬¸ìì—´ì¸ ê²½ìš° ê·¸ëŒ€ë¡œ ì‚¬ìš©
                  else:
                      final_response_content += chunk.content
                      print(chunk.content, end="", flush=True)
              await asyncio.sleep(0)

          print("\n\n--- Complete Final Response ---")
          print(final_response_content)


  if __name__ == "__main__":
      asyncio.run(main())
  ```
</example>