---
description: 
globs: **/*.py
alwaysApply: false
---
<prerequisites>
- Use Langchain AWS (`langchain-aws`) library
- Use `<project_root>/app.py` file as main event loop entry point
</prerequisites>

<chat-life-cycle-hooks>
- The `on_chat_start` decorator is used to define a hook that is called when a new chat session is created.
- The `on_message` decorator is used to define a hook that is called when a new message is received from the user.
- The `on_chat_end` decorator is used to define a hook that is called when the chat session ends either because the user disconnected or started a new chat session.
- The `on_chat_resume` decorator is used to define a hook that is called when a user resumes a chat session that was previously disconnected. This can only happen if `authentication` and `data persistence` are enabled.
</chat-life-cycle-hooks>

<async-invoke-message>
- Use async/await for message
- Example:
```python
from langchain_openai import ChatOpenAI
from langchain.prompts import ChatPromptTemplate
from langchain.schema import StrOutputParser
from langchain.schema.runnable import Runnable
from langchain.schema.runnable.config import RunnableConfig
from typing import cast

import chainlit as cl


@cl.on_chat_start
async def on_chat_start():
    model = ChatOpenAI(streaming=True)
    prompt = ChatPromptTemplate.from_messages(
        [
            (
                "system",
                "You're a very knowledgeable historian who provides accurate and eloquent answers to historical questions.",
            ),
            ("human", "{question}"),
        ]
    )
    runnable = prompt | model | StrOutputParser()
    cl.user_session.set("runnable", runnable)


@cl.on_message
async def on_message(message: cl.Message):
    runnable = cast(Runnable, cl.user_session.get("runnable"))  # type: Runnable

    msg = cl.Message(content="")

    async for chunk in runnable.astream(
        {"question": message.content},
        config=RunnableConfig(callbacks=[cl.LangchainCallbackHandler()]),
    ):
        await msg.stream_token(chunk)

    await msg.send()
```
</async-invoke-message>

<multi-modality>
- Chainlit supports the multi-modal functionality to access these attached files through the `cl.on_message` decorated function.
- Image and Text files (e.g. PDF, Markdown) can be attached.
- Example:
```python
@cl.on_message
async def on_message(msg: cl.Message):
    if not msg.elements:
        await cl.Message(content="No file attached").send()
        return

    # Processing images exclusively
    images = [file for file in msg.elements if "image" in file.mime]

    # Read the first image
    with open(images[0].path, "r") as f:
        pass

    await cl.Message(content=f"Received {len(images)} image(s)").send()
```
</multi-modality>

<streaming-message>
- Example:
```python
@cl.on_message
async def main(message: cl.Message):
    message_history = cl.user_session.get("message_history")
    message_history.append({"role": "user", "content": message.content})

    msg = cl.Message(content="")

    stream = await client.chat.completions.create(
        messages=message_history, stream=True, **settings
    )

    async for part in stream:
        if token := part.choices[0].delta.content or "":
            await msg.stream_token(token)

    message_history.append({"role": "assistant", "content": msg.content})
    await msg.update()
```
</streaming-message>

<step-component>
- The `Step` class is a Python Context Manager that can be used to create steps in your chainlit app. The step is created when the context manager is entered and is updated to the client when the context manager is exited.
- Example:
```python
@cl.step(type="tool")
async def tool():
    # Simulate a running task
    await cl.sleep(2)

    return "Response from the tool!"


@cl.on_message
async def main(message: cl.Message):
    # Call the tool
    tool_res = await tool()

    # Send the final answer.
    await cl.Message(content="This is the final answer").send()
```
</step-component>

<hook-on-chat-resume>
- Example:
```python
@cl.on_chat_resume
async def on_chat_resume(thread: ThreadDict):
    # set commands if commands
    if COMMANDS:
        await cl.context.emitter.set_commands(COMMANDS)

    # restore the message history from the thread
    message_history = cl.user_session.get("message_history", [])
    for message in (m for m in thread["steps"]):
        if message["type"] == "user_message":
            message_history.append(
                {"role": "user", "content": message['output']}
            )
        else:
            message_history.append(
                {"role": "assistant", "content": message["output"]}
            )
    logger.info(f"Restored message_history: {len(message_history)}")
    cl.user_session.set("message_history", message_history)
```
</hook-on-chat-resume>